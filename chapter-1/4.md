# 1.4 Retrieval-Augmented Generation (RAG)

## ðŸ”¹ 1.4.1 Why Do We Need RAG?

LLMs are powerful but limited:

- They **donâ€™t know fresh information** beyond their training cutoff.
- They may **hallucinate** (make up answers).
- They canâ€™t store your **organizationâ€™s private knowledge** inside the model.

ðŸ‘‰ RAG solves this:  
It **retrieves relevant information** from an external source (database, documents, APIs) before generating the final answer.

Think of it like:

- LLM = the **brain**
- Vector database = the **memory**

---

## ðŸ”¹ 1.4.2 How RAG Works (Step by Step)

1. **User Query** â†’ "Whatâ€™s our refund policy for premium users?"
2. **Embedding** â†’ Convert query into a vector.
3. **Vector Search** â†’ Find closest matches in knowledge base (docs, FAQs, DB).
4. **Context Injection** â†’ Insert retrieved chunks into the LLM prompt.
5. **LLM Response** â†’ Generate an answer grounded in retrieved facts.

ðŸ‘‰ Architecture Diagram:  
[![RAG Architecture](https://www.pinecone.io/learn/img/rag-architecture.png)](https://www.pinecone.io/learn/retrieval-augmented-generation/)

---

## ðŸ”¹ 1.4.3 Key Components of RAG

- **Embeddings Model** â†’ Converts queries & docs into vectors.
- **Vector Database** â†’ Stores embeddings (Pinecone, Weaviate, FAISS, Milvus).
- **Retriever** â†’ Finds top-N relevant chunks.
- **LLM** â†’ Generates answer conditioned on retrieved data.

ðŸ‘‰ Reference: [Pinecone RAG Guide](https://www.pinecone.io/learn/retrieval-augmented-generation/).

---

## ðŸ”¹ 1.4.4 Practical Lab: Build a Mini RAG

### Step 1: Install dependencies

```bash
pip install openai faiss-cpu tiktoken
```

**Step 2: Index documents into FAISS**

```python
import faiss
import numpy as np
from openai import OpenAI

client = OpenAI(api_key="your-key")

def get_embedding(text):
    return client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    ).data[0].embedding

docs = [
    "Refund policy: Premium users have 30 days to request refunds.",
    "Standard users cannot request refunds.",
    "Support is available 24/7 for premium members."
]

embeddings = [get_embedding(doc) for doc in docs]
dim = len(embeddings[0])
index = faiss.IndexFlatL2(dim)
index.add(np.array(embeddings).astype("float32"))

```

**Step 3: Query the index**

```python
query = "How long do premium users have to refund?"
q_emb = np.array([get_embedding(query)]).astype("float32")

D, I = index.search(q_emb, k=2)
print("Top results:")
for idx in I[0]:
    print(docs[idx])

```

Expected output:

```pgsql
Refund policy: Premium users have 30 days to request refunds.
Support is available 24/7 for premium members.
```

**Step 4: Generate grounded answer**

```python
context = " ".join([docs[i] for i in I[0]])

prompt = f"""
Answer the question based on the context below:

Context: {context}
Question: {query}
"""

resp = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": prompt}]
)

print(resp.choices[0].message.content)
```

Output â†’
Premium users have 30 days to request refunds. âœ… (no hallucination)

**ðŸ”¹ 1.4.5 RAG Patterns (Architectâ€™s Perspective)**

- Simple RAG â†’ One-shot retrieval (like lab above).
- Multi-hop RAG â†’ Break query into sub-questions, retrieve multiple times.
- Hybrid RAG â†’ Combine vector search + keyword search.
- Agentic RAG â†’ LLM decides when and how to query memory.

ðŸ‘‰ Great article: [Agentic RAG](https://blog.langchain.dev/agentic-rag/).

**ðŸ”¹ 1.4.6 Why RAG Matters for AI Architects**

- Keeps models up-to-date without retraining.
- Controls hallucination by grounding answers.
- Secures enterprise knowledge (data stays in your vector DB, not inside LLM).
- Scales well: you can continuously add new docs, APIs, datasets.

**âœ… Checkpoint for You**

- Can you describe how embeddings + vector DBs work together in RAG?
- Do you understand the flow: Query â†’ Embed â†’ Search â†’ Inject â†’ Answer?
- Can you run the FAISS lab and get real results?

If yes â†’ youâ€™re ready for 1.5: Prompt Engineering for AI Architects (making prompts work with retrieved knowledge).
