# 1.3 Tokenization, Embeddings & Vector Representations

Understanding **how text becomes math** is critical for any AI Architect.  
LLMs don’t “see” words → they see numbers.

This section explains **tokenization** (breaking text), **embeddings** (converting to vectors), and why this matters for **search, retrieval, personalization, and reasoning pipelines**.

---

## 🔹 1.3.1 What is Tokenization?

- Text → split into smaller units called **tokens**.
- A token is not always a word. It can be:
  - A whole word (`dog`)
  - A subword (`comput`, `er`)
  - Even punctuation (`!`, `?`)

👉 Example:  
`"AI Architecting rocks!"` → `[ "AI", " Arch", "itect", "ing", " rocks", "!" ]`

Why it matters:

- **Billing**: APIs like OpenAI charge **per token**.
- **Context window**: Models can only process limited tokens (e.g., 8k, 32k).

👉 Try it yourself: [OpenAI Tokenizer](https://platform.openai.com/tokenizer).

---

## 🔹 1.3.2 Embeddings (Turning Tokens into Vectors)

- Each token is mapped to a **vector** = list of numbers (e.g., `[0.12, -0.89, ...]`).
- These vectors live in **high-dimensional space** (like coordinates in 1,536 dimensions).
- Vectors with **similar meanings are closer together**.

👉 Example:

- “king” and “queen” → vectors are close.
- “king” - “man” + “woman” ≈ “queen”.

This property powers:

- **Semantic search**
- **Recommendations**
- **Knowledge retrieval (RAG)**

👉 Reference: [Vector Embeddings Explained](https://huggingface.co/docs/transformers/index).

---

## 🔹 1.3.3 Practical Labs

### Lab 1: Tokenization Basics

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")

text = "AI Architecting is powerful!"
print("Original text:", text)
print("Tokens:", tokenizer.tokenize(text))
print("Token IDs:", tokenizer.encode(text))


```

**Architect’s insight:**
Different models use different tokenizers → same text can cost differently in tokens.

**Lab 2: Generate Embeddings (OpenAI Example)**

```python
from openai import OpenAI
client = OpenAI(api_key="your-key")

resp = client.embeddings.create(
    model="text-embedding-3-small",
    input="AI Architecting is the future!"
)

vector = resp.data[0].embedding
print("Vector length:", len(vector))
print("First 10 dims:", vector[:10])
```

- Output = a vector of ~1,536 numbers.

- Architect’s use: Store these in a vector database (Pinecone, Weaviate, FAISS) to enable semantic search.

👉 Free alternative: [Sentence Transformers](https://www.sbert.net/).

**Lab 3: Semantic Similarity**

```python
from openai import OpenAI
import numpy as np

client = OpenAI(api_key="your-key")

def get_embedding(text):
    return client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    ).data[0].embedding

v1 = get_embedding("AI Architect")
v2 = get_embedding("Machine Learning Designer")
v3 = get_embedding("Coffee brewing")

def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

print("AI Architect vs ML Designer:", cosine_similarity(v1, v2))
print("AI Architect vs Coffee:", cosine_similarity(v1, v3))

```

- Expect high similarity between "AI Architect" & "ML Designer".
- Expect low similarity between "AI Architect" & "Coffee brewing".

This is exactly how LLM-powered search & recommendations work.

**1.3.4 Why This Matters for AI Architects**

- Tokenization → Helps you estimate costs & limits.
- Embeddings → The backbone of RAG, personalization, and semantic search.
- Vector Databases → Core infra for AI architectures (your job is to pick + design integration).
- Similarity Search → You’ll design pipelines that rely on embedding similarity to retrieve the right knowledge before the LLM answers.

👉 Suggested Reading:

[The Illustrated Word2Vec](https://jalammar.github.io/illustrated-word2vec/) (classic, explains embeddings simply).
[Pinecone](https://www.pinecone.io/learn/vector-database/): What are Vector Databases?.

✅ **Checkpoint for You**

- Can you explain why embeddings are useful?
- Do you know how tokenization affects costs and context limits?
- Can you sketch (on paper) a simple RAG pipeline: User query → embedding → vector search → LLM?

If yes → you’re ready for **1.4: Architecting Retrieval-Augmented Generation (RAG)**.
If not → revisit the tokenization tool + play with embedding labs.
