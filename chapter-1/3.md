# 1.3 Tokenization, Embeddings & Vector Representations

Understanding **how text becomes math** is critical for any AI Architect.  
LLMs donâ€™t â€œseeâ€ words â†’ they see numbers.

This section explains **tokenization** (breaking text), **embeddings** (converting to vectors), and why this matters for **search, retrieval, personalization, and reasoning pipelines**.

---

## ğŸ”¹ 1.3.1 What is Tokenization?

- Text â†’ split into smaller units called **tokens**.
- A token is not always a word. It can be:
  - A whole word (`dog`)
  - A subword (`comput`, `er`)
  - Even punctuation (`!`, `?`)

ğŸ‘‰ Example:  
`"AI Architecting rocks!"` â†’ `[ "AI", " Arch", "itect", "ing", " rocks", "!" ]`

Why it matters:

- **Billing**: APIs like OpenAI charge **per token**.
- **Context window**: Models can only process limited tokens (e.g., 8k, 32k).

ğŸ‘‰ Try it yourself: [OpenAI Tokenizer](https://platform.openai.com/tokenizer).

---

## ğŸ”¹ 1.3.2 Embeddings (Turning Tokens into Vectors)

- Each token is mapped to a **vector** = list of numbers (e.g., `[0.12, -0.89, ...]`).
- These vectors live in **high-dimensional space** (like coordinates in 1,536 dimensions).
- Vectors with **similar meanings are closer together**.

ğŸ‘‰ Example:

- â€œkingâ€ and â€œqueenâ€ â†’ vectors are close.
- â€œkingâ€ - â€œmanâ€ + â€œwomanâ€ â‰ˆ â€œqueenâ€.

This property powers:

- **Semantic search**
- **Recommendations**
- **Knowledge retrieval (RAG)**

ğŸ‘‰ Reference: [Vector Embeddings Explained](https://huggingface.co/docs/transformers/index).

---

## ğŸ”¹ 1.3.3 Practical Labs

### Lab 1: Tokenization Basics

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")

text = "AI Architecting is powerful!"
print("Original text:", text)
print("Tokens:", tokenizer.tokenize(text))
print("Token IDs:", tokenizer.encode(text))


```

**Architectâ€™s insight:**
Different models use different tokenizers â†’ same text can cost differently in tokens.

**Lab 2: Generate Embeddings (OpenAI Example)**

```python
from openai import OpenAI
client = OpenAI(api_key="your-key")

resp = client.embeddings.create(
    model="text-embedding-3-small",
    input="AI Architecting is the future!"
)

vector = resp.data[0].embedding
print("Vector length:", len(vector))
print("First 10 dims:", vector[:10])
```

- Output = a vector of ~1,536 numbers.

- Architectâ€™s use: Store these in a vector database (Pinecone, Weaviate, FAISS) to enable semantic search.

ğŸ‘‰ Free alternative: [Sentence Transformers](https://www.sbert.net/).

**Lab 3: Semantic Similarity**

```python
from openai import OpenAI
import numpy as np

client = OpenAI(api_key="your-key")

def get_embedding(text):
    return client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    ).data[0].embedding

v1 = get_embedding("AI Architect")
v2 = get_embedding("Machine Learning Designer")
v3 = get_embedding("Coffee brewing")

def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

print("AI Architect vs ML Designer:", cosine_similarity(v1, v2))
print("AI Architect vs Coffee:", cosine_similarity(v1, v3))

```

- Expect high similarity between "AI Architect" & "ML Designer".
- Expect low similarity between "AI Architect" & "Coffee brewing".

This is exactly how LLM-powered search & recommendations work.

**1.3.4 Why This Matters for AI Architects**

- Tokenization â†’ Helps you estimate costs & limits.
- Embeddings â†’ The backbone of RAG, personalization, and semantic search.
- Vector Databases â†’ Core infra for AI architectures (your job is to pick + design integration).
- Similarity Search â†’ Youâ€™ll design pipelines that rely on embedding similarity to retrieve the right knowledge before the LLM answers.

ğŸ‘‰ Suggested Reading:

[The Illustrated Word2Vec](https://jalammar.github.io/illustrated-word2vec/) (classic, explains embeddings simply).
[Pinecone](https://www.pinecone.io/learn/vector-database/): What are Vector Databases?.

âœ… **Checkpoint for You**

- Can you explain why embeddings are useful?
- Do you know how tokenization affects costs and context limits?
- Can you sketch (on paper) a simple RAG pipeline: User query â†’ embedding â†’ vector search â†’ LLM?

If yes â†’ youâ€™re ready for **1.4: Architecting Retrieval-Augmented Generation (RAG)**.
If not â†’ revisit the tokenization tool + play with embedding labs.
