# Chapter 1: Modern LLM System Architecture for Engineers

---

## üéØ Objectives

- Understand how **real-world LLM systems** are structured.
- Learn the **main building blocks** (beyond calling `openai.chat()` APIs).
- Set up a **mini-stack** you can run and extend locally.

This chapter is split into portions for easier reading and hands-on practice.

---

## 1. Why You Need a System View

After 7 years as a fullstack engineer, you know that apps aren‚Äôt just ‚ÄúReact + API.‚Äù They‚Äôre designed with:

- **Data flow** (inputs/outputs),
- **Orchestration** (middleware, workflows),
- **Monitoring** (logs, metrics),
- **Scaling** (cost & latency tradeoffs).

LLM-powered systems are **no different** ‚Äî but the ‚Äúbackend‚Äù logic is replaced by **a reasoning engine** (the model) plus **retrieval and tool layers**.

---

## 2. Core Building Blocks of LLM Apps

Almost every AI product today has 3 pillars:

### 2.1 Base Model Access

- **Hosted models**: OpenAI GPT-4o, Anthropic Claude, Google Gemini, Azure OpenAI.
- **Open-source models**: Meta LLaMA 3, Mistral, Mixtral.
- **Serving runtimes**:
  - _Ollama_ ‚Äì simple local runner for Mac/Linux/WSL ([Ollama](https://ollama.ai/))
  - _vLLM_ ‚Äì high-throughput inference server ([vLLM](https://vllm.ai/))

üí° _Why it matters_: Choosing model + runtime determines **latency, cost, deployment feasibility**.

---

### 2.2 Context Enrichment (RAG & Memory)

- **RAG (Retrieval-Augmented Generation)** = give the LLM extra context at query time.
- Uses **vector DBs** like [Weaviate](https://weaviate.io/), [pgvector](https://github.com/pgvector/pgvector), [Chroma](https://www.trychroma.com/).
- Memory types:
  - _Short-term_: The conversation context (tokens in prompt).
  - _Long-term_: External storage (vector DBs, SQL DBs, files).

üí° _Why it matters_: Without RAG, models ‚Äúhallucinate.‚Äù With it, they can answer based on your org‚Äôs data.

---

### 2.3 Orchestration / Agent Layer

- Orchestrators manage **flows of calls** between model + tools.
- Example frameworks:
  - [LangGraph](https://langchain-ai.github.io/langgraph/) ‚Äì graph/state-machine approach for LLM apps.
  - [Haystack Agents](https://haystack.deepset.ai/).
- Techniques:
  - **Tool calling** ‚Äì LLM decides when to call APIs.
  - **Planning & reasoning loops** ‚Äì ReAct, Reflexion, multi-agent patterns.
  - **Guardrails** ‚Äì enforce JSON schemas, filter bad output, retry on errors.

üí° _Why it matters_: Without orchestration, LLM apps are brittle and unpredictable. With orchestration, you get **reliability + maintainability**.

---

## 3. Expert Perspective: Lilian Weng (OpenAI Research Lead)

üëâ Blog: [LLM-powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agents/)

**Summary (so you don‚Äôt lose the knowledge if the page breaks):**

- An agent = LLM + **Planning** + **Memory** + **Tool use**.
- **Planning**: break tasks into smaller steps, use self-reflection.
- **Memory**: in-context (short) vs external DB (long).
- **Tool use**: call APIs, search, calculators, other models.
- Most real apps combine **RAG + tool calling + planning** for reliability.

---

## ‚úÖ Checklist (end of Part A)

- [ ] I understand that an **LLM system ‚â† just the model** ‚Äî it‚Äôs model + context + orchestration.
- [ ] I can explain why **RAG reduces hallucination** and why orchestration matters.
- [ ] I‚Äôve read Lilian Weng‚Äôs summary or blog post.

---

‚û°Ô∏è Next in **Part B**: Reference Architectures + Hands-On Labs (Ollama, pgvector RAG, LangGraph quickstart).
