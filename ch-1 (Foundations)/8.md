# 1.8 Open-Source Projects & Frameworks — Deep Dive (Developer + Architect)

This chapter surveys the **most useful open-source projects and frameworks** you’ll actually use as an AI Architect. It focuses on **practical tradeoffs**, recommended starter stacks, and concrete quickstarts so you can run things locally and iterate quickly. All links are directly copiable; I also cite authoritative docs and repo pages for each recommendation.

---

## 1.8.1 Why choose OSS tools (short)

Open-source frameworks let you:

- Inspect how orchestration, retrieval, and safety are implemented.
- Run **local prototypes** without cloud costs or vendor lock-in.
- Choose hybrid architectures (local inference + hosted APIs) depending on constraints.  
  Practical note: many teams use OSS for prototyping and a managed cloud product for production scale. citeturn0search12turn0search1

---

## 1.8.2 Orchestration & Agent Frameworks (what & when)

### LangChain (Python / JS)

- **What:** Composable building blocks (LLM wrappers, chains, agents, memory). Great for RAG + tool-using agents and rapid prototyping. citeturn0search0turn0search4
- **When to use:** prototypes to production pipelines that need lots of integrations (databases, retrievals, tools).
- **Quickstart (Python):**

```bash
pip install langchain openai
```

```python
from langchain import OpenAI, LLMChain, PromptTemplate
llm = OpenAI(model="gpt-4o-mini", temperature=0)
prompt = PromptTemplate("Summarize: {text}")
chain = LLMChain(llm=llm, prompt=prompt)
print(chain.run({"text":"Your document here"}))
```

### AutoGen (Microsoft) — multi-agent focus

- **What:** Framework optimized for **multi-agent conversations** and event-driven agentic workflows. Strong for experiments where multiple specialized agents must cooperate. citeturn0search1turn0search5
- **When to use:** prototyping multi-agent orchestration, human-in-the-loop flows, or research into agent behaviors.
- **Quickstart hint:** check the AutoGen repo examples to build AssistantAgent/UserProxyAgent flows. citeturn0search9

### Semantic Kernel (Microsoft) — enterprise orchestration

- **What:** SDK for building AI orchestration with planner & plugin primitives; supports Python, .NET, Java. Good for enterprise-grade integrations. citeturn0search2turn0search18
- **When to use:** enterprise apps requiring robust connectors (Azure/OpenAI/Hugging Face) and planners.
- **Quickstart hint:** follow Microsoft Learn quickstart to build a planner + skill plugin. citeturn0search10

### LlamaIndex / LlamaCloud (data adapter & orchestration)

- **What:** Data-indexing & orchestration layer that focuses on connecting unstructured enterprise data to LLMs (indexes, adapters, response synth). Useful for complex LLM + data workflows. citeturn2search0turn2search8
- **When to use:** complex document-heavy products (financial docs, legal, long-form knowledge bases).

### Haystack (deepset) — production RAG & agents

- **What:** End-to-end framework for production RAG, QA, and tool-using agents. Strong on deployability and enterprise integrations. citeturn2search13turn2search21
- **When to use:** you need an infrastucture-first RAG stack with pipelines, GPU support, and deployment playbooks.

---

## 1.8.3 Vector Stores & Retrieval (practical choices)

### Chroma — developer-friendly local vector DB

- **What:** Simple, local-first vector DB with Python/JS clients. Great for fast iteration. citeturn1search2turn1search6
- **Good for:** prototype/dev machines & small-production workloads.

### pgvector — Postgres extension (ops-friendly)

- **What:** Keep vectors in Postgres; transactional, integrates with existing schemas. Works with Postgres providers (Supabase, Neon). citeturn1search3turn1search11
- **Good for:** teams that want single-store ops, ACID guarantees, and SQL access for embeddings.

### Weaviate — AI-native DB with RAG support

- **What:** Open-source, scalable vector DB with integrated RAG patterns and graph extensions. Great docs and learning center. citeturn4search16turn4search6
- **Good for:** index-heavy RAG at scale and multimodal data.

### Milvus & FAISS — scale and performance

- **FAISS:** library by Meta for very efficient nearest-neighbor search (GPU-accelerated). Use when you need raw performance. citeturn2search3turn2search11
- **Milvus:** purpose-built vector DB that scales horizontally; useful for production with high volumes. citeturn2search2turn2search6

---

## 1.8.4 Serving & Inference Runtimes

### vLLM — high-throughput, OpenAI-compatible server

- **What:** Optimized inference server with an OpenAI-compatible HTTP interface (drop-in for existing OpenAI-style clients). Good batching & throughput. citeturn1search0turn1search4
- **Quickstart:** `pip install vllm` then `python -m vllm.entrypoints.openai.api_server --model <hf-model>` — the server implements Chat/Completions APIs. citeturn1search8turn1search12

### Ollama — local-first LLM runner with embedding API

- **What:** Desktop/server tool to run LLMs and embedding models locally with a REST API (`/api/embeddings`, `/api/chat`). Great for privacy & local prototyping. citeturn1search1turn1search21
- **Quickstart:** `ollama pull llama3` then `ollama run llama3` and use `http://localhost:11434/api/*` endpoints. citeturn1search21turn1search13

### Notes on tiny runtimes

- **llama.cpp** and similar projects enable CPU-only local inference for small models. Use for offline demos or embedding-only tasks when GPUs are not available.

---

## 1.8.5 Observability & Evaluation (must-have for production)

- **Langfuse** — open-source tracing & observability for LLM apps; captures inputs/outputs, tool calls, latencies. Useful for debugging RAG/agent flows. citeturn3search2turn3search10
- **Arize / Phoenix** — enterprise-grade observability + Phoenix OSS for tracing/eval. Designed for LLM evaluation and monitoring. citeturn3search0turn3search17
- **Weights & Biases (W&B)** — experiment tracking, model evaluation, hosted inference. Integrates with model training and some inference workflows. citeturn3search1
- **DeepEval / RAGAS / OpenAI Evals** — frameworks for systematic evaluation of RAG/agent outputs; important for QA pipelines and regression testing. citeturn3search3turn3search6turn3search7

---

## 1.8.6 Starter stacks (recommended by use-case)

1. **Local Prototype (fastest)**

   - Ollama (local LLM + embeddings) + Chroma (local vector DB) + LangChain (agent prototype).
   - Why: minimal infra, full local control, low cost. citeturn1search21turn1search2

2. **Internal Production (team use)**

   - vLLM serving (OpenAI-compatible) or managed OpenAI/Azure + pgvector (Postgres) + LangChain/Haystack + Langfuse for tracing.
   - Why: production throughput + transactional storage + observability. citeturn1search0turn1search3turn2search13

3. **Enterprise / Regulated**
   - Semantic Kernel or LlamaIndex for data governance + Milvus/Weaviate for scale + Arize for observability + strict guardrails and human-in-loop controls.
   - Why: enterprise connectors, governance, auditing & compliance. citeturn0search2turn2search6turn3search8

---

## 1.8.7 Concrete Labs / Commands (copy-paste)

### Quick: Run Ollama + Chroma (local prototype)

```bash
# 1) Install Chroma (Python)
pip install chromadb

# 2) Pull and run a small model with Ollama (after install from https://ollama.com)
ollama pull llama3
ollama run llama3

# 3) Use Ollama embeddings endpoint
curl http://localhost:11434/api/embed -d '{"model":"nomic-embed-text","input":"Your text here"}'
```

### Quick: Run vLLM server (OpenAI-compatible)

```bash
pip install vllm
python -m vllm.entrypoints.openai.api_server --model NousResearch/Meta-Llama-3-8B-Instruct --host 0.0.0.0 --port 8000
```

Then call the server with any OpenAI-compatible SDK. citeturn1search4turn1search16

### Quick: Stand up pgvector (docker)

```bash
docker run --name pg -e POSTGRES_PASSWORD=postgres -p 5432:5432 -d ankane/pgvector
```

Then use `pgvector` client libraries to insert embeddings. citeturn1search3

---

## 1.8.8 Tradeoffs & Best Practices (architect-level)

1. **Prototype locally, scale selectively** — start with Ollama + Chroma; switch to vLLM/managed services when latency/throughput demands increase. citeturn1search21turn1search0
2. **Use the right vector store** — `pgvector` if you prefer SQL ops & transactional semantics; `Milvus`/`Weaviate`/`FAISS` when you need scale and specialized indexing. citeturn1search3turn2search6turn2search3
3. **Observability first** — log prompts, retrieved docs, model responses, token usage; use Langfuse/Arize for traceability. Evaluation frameworks (DeepEval/OpenAI Evals) should be part of CI. citeturn3search2turn3search3
4. **Guardrails** — validate structured outputs, enforce JSON schemas, and keep human-in-loop for critical decisions.
5. **Cost controls** — use smaller models for background agents; cache answers; use rerankers to reduce generation token use. citeturn4search3

---

## 1.8.9 Links & Further Reading (copyable)

- LangChain docs: https://python.langchain.com/docs/ citeturn0search16
- AutoGen (Microsoft): https://github.com/microsoft/autogen citeturn0search1
- Semantic Kernel (Microsoft Learn): https://learn.microsoft.com/en-us/semantic-kernel/ citeturn0search2
- LlamaIndex docs: https://docs.llamaindex.ai/ citeturn2search0
- Haystack (deepset): https://haystack.deepset.ai/ citeturn2search13
- Ollama API & models: https://ollama.com/ and https://ollama.readthedocs.io/en/api/ citeturn1search21turn1search1
- vLLM docs: https://docs.vllm.ai/ citeturn1search0
- Chroma docs: https://docs.trychroma.com/ citeturn1search2
- pgvector GitHub: https://github.com/pgvector/pgvector citeturn1search3
- Weaviate docs: https://weaviate.io/docs citeturn4search16
- FAISS docs: https://faiss.ai/ citeturn2search3
- Langfuse: https://langfuse.com/docs citeturn3search2
- Arize: https://arize.com/docs/ax citeturn3search0
- DeepEval: https://github.com/confident-ai/deepeval citeturn3search3

---

## 1.8.10 Practical next steps (for you, the engineer)

1. Pick a starter stack: _Ollama + Chroma + LangChain_ and implement a 1-week RAG prototype with two real docs. citeturn1search21turn1search2
2. Add observability: integrate Langfuse or simple trace logging to capture prompts, retrieved docs, and responses. citeturn3search2
3. Evaluate: write a few DeepEval or OpenAI Evals tests for your RAG outputs to catch regressions. citeturn3search3turn3search6

---

_End of Chapter 1.8 — Open-Source Projects & Frameworks._
