# 1.4 Retrieval-Augmented Generation (RAG)

## 🔹 1.4.1 Why Do We Need RAG?

LLMs are powerful but limited:

- They **don’t know fresh information** beyond their training cutoff.
- They may **hallucinate** (make up answers).
- They can’t store your **organization’s private knowledge** inside the model.

👉 RAG solves this:  
It **retrieves relevant information** from an external source (database, documents, APIs) before generating the final answer.

Think of it like:

- LLM = the **brain**
- Vector database = the **memory**

---

## 🔹 1.4.2 How RAG Works (Step by Step)

1. **User Query** → "What’s our refund policy for premium users?"
2. **Embedding** → Convert query into a vector.
3. **Vector Search** → Find closest matches in knowledge base (docs, FAQs, DB).
4. **Context Injection** → Insert retrieved chunks into the LLM prompt.
5. **LLM Response** → Generate an answer grounded in retrieved facts.

👉 Architecture Diagram:  
[![RAG Architecture](https://www.pinecone.io/learn/img/rag-architecture.png){:target="_blank"}](https://www.pinecone.io/learn/retrieval-augmented-generation/)

---

## 🔹 1.4.3 Key Components of RAG

- **Embeddings Model** → Converts queries & docs into vectors.
- **Vector Database** → Stores embeddings (Pinecone, Weaviate, FAISS, Milvus).
- **Retriever** → Finds top-N relevant chunks.
- **LLM** → Generates answer conditioned on retrieved data.

👉 Reference: [Pinecone RAG Guide](https://www.pinecone.io/learn/retrieval-augmented-generation/){:target="_blank"}.

---

## 🔹 1.4.4 Practical Lab: Build a Mini RAG

### Step 1: Install dependencies

```bash
pip install openai faiss-cpu tiktoken
```

**Step 2: Index documents into FAISS**

```python
import faiss
import numpy as np
from openai import OpenAI

client = OpenAI(api_key="your-key")

def get_embedding(text):
    return client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    ).data[0].embedding

docs = [
    "Refund policy: Premium users have 30 days to request refunds.",
    "Standard users cannot request refunds.",
    "Support is available 24/7 for premium members."
]

embeddings = [get_embedding(doc) for doc in docs]
dim = len(embeddings[0])
index = faiss.IndexFlatL2(dim)
index.add(np.array(embeddings).astype("float32"))

```

**Step 3: Query the index**

```python
query = "How long do premium users have to refund?"
q_emb = np.array([get_embedding(query)]).astype("float32")

D, I = index.search(q_emb, k=2)
print("Top results:")
for idx in I[0]:
    print(docs[idx])

```

Expected output:

```pgsql
Refund policy: Premium users have 30 days to request refunds.
Support is available 24/7 for premium members.
```

**Step 4: Generate grounded answer**

```python
context = " ".join([docs[i] for i in I[0]])

prompt = f"""
Answer the question based on the context below:

Context: {context}
Question: {query}
"""

resp = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": prompt}]
)

print(resp.choices[0].message.content)
```

Output →
Premium users have 30 days to request refunds. ✅ (no hallucination)

**🔹 1.4.5 RAG Patterns (Architect’s Perspective)**

- Simple RAG → One-shot retrieval (like lab above).
- Multi-hop RAG → Break query into sub-questions, retrieve multiple times.
- Hybrid RAG → Combine vector search + keyword search.
- Agentic RAG → LLM decides when and how to query memory.

👉 Great article: [Agentic RAG](https://blog.langchain.dev/agentic-rag/){:target="_blank"}.

**🔹 1.4.6 Why RAG Matters for AI Architects**

- Keeps models up-to-date without retraining.
- Controls hallucination by grounding answers.
- Secures enterprise knowledge (data stays in your vector DB, not inside LLM).
- Scales well: you can continuously add new docs, APIs, datasets.

**✅ Checkpoint for You**

- Can you describe how embeddings + vector DBs work together in RAG?
- Do you understand the flow: Query → Embed → Search → Inject → Answer?
- Can you run the FAISS lab and get real results?

If yes → you’re ready for 1.5: Prompt Engineering for AI Architects (making prompts work with retrieved knowledge).


