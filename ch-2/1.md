# Chapter 2 — Core Principles of AI System Architecture (for Senior Developers → AI Architect)

**Scope & intent:** practical, expert-level system design guidance you can apply immediately.  
This chapter assumes you already know web/backend fundamentals (APIs, Docker, CI/CD) and focuses on _what changes_ when the “brain” of the system is an LLM or a set of LLM-based agents. I include concrete patterns, trade-offs, and references to highly credible sources used by production teams.

---

## 2.0 Why this chapter matters (short)

Designing LLM-powered systems is NOT just “replace the business logic with a model.” It requires new architectural primitives (retrieval, orchestration, memory, evaluation) and new non-functional concerns (token cost, hallucination risk, prompt/agent governance). Many enterprises that treat LLMs as drop-in components fail to scale or operate them reliably. :contentReference[oaicite:0]{index=0}

---

## 2.1 Fundamental differences vs. classic server-side systems

1. **Probabilistic outputs vs deterministic APIs**

   - LLMs return plausible-sounding text, not guaranteed correct values. Architecture must include verification & fallback paths. (Design consequence: add validation & evidence chains before relying on any generated output.)

2. **Context-window economics (tokens are first-class resources)**

   - Tokens determine cost and correctness (context truncation breaks reasoning). Architect for token budgets: chunking, summarization, and retrieval. (Design consequence: prefer short prompts + RAG vs stuffing raw docs into prompts.)

3. **Retrieval-as-memory (externalize state)**

   - For long-term or updated knowledge, store embeddings in vector DBs rather than fine-tuning models for every knowledge update. This enables fast updates without retraining. :contentReference[oaicite:1]{index=1}

4. **Multi-model, multi-runtime stacks**

   - Production uses different models for distinct needs: tiny models for routing/caching, medium models for extraction, large models for high-quality synthesis. Architect for orchestration and model selection. (Design consequence: routing layer that picks model & runtime per request.)

5. **Emergent agentic behavior**
   - When you give LLMs tools and loops, they can plan and act — powerful but risky. Use strict guardrails and bounded autonomy. :contentReference[oaicite:2]{index=2}

---

## 2.2 Core architectural primitives (patterns you will reuse)

### 2.2.1 Retrieval Layer (RAG pattern)

- **Responsibility:** turn query → top-k relevant context chunks (embeddings → vector search → rerank).
- **Components:** embedding model, vector DB (Chroma/pgvector/Weaviate/FAISS), retriever, reranker.
- **Design notes:** chunk size & stride, hybrid search (BM25 + vector), reranking for precision. :contentReference[oaicite:3]{index=3}

### 2.2.2 Orchestration / Agent Layer

- **Responsibility:** manage multi-step flows, tool calls, retries, and state transitions.
- **Patterns:** linear chain (simple), graph/state-machine (complex workflows), planner + executor (agentic). Use frameworks (LangChain, AutoGen) for higher-level constructs. :contentReference[oaicite:4]{index=4}

### 2.2.3 Inference/Serving Layer

- **Responsibility:** efficient, scalable model inference; model selection & routing; streaming responses.
- **Runtimes:** vLLM for high-throughput, Ollama/llama.cpp for local development, managed API for rapid iteration. Design for batching, streaming, and model sharding. :contentReference[oaicite:5]{index=5}

### 2.2.4 Evaluation & Observability Layer (LLMOps)

- **Responsibility:** measure hallucinations, latency, cost, and drift; enable gated deployments (A/B + human-in-loop).
- **Tools/ideas:** trace all prompts + retrieval traces, use eval harnesses, integrate Langfuse/Arize/W&B. Production CI must include eval suites. :contentReference[oaicite:6]{index=6}

### 2.2.5 Governance/Guardrails Layer

- **Responsibility:** enforce policies, privacy, schema validation, and fail-safe human handoffs.
- **Practices:** prompt trust tiers, input sanitization, signed/sandboxed tool calls, JSON-schema output validation, runtime circuit breakers. :contentReference[oaicite:7]{index=7}

---

## 2.3 Typical production topology (concise blueprint)

[Client UI / API]
↕
[API Gateway / Auth / Rate Limits]
↕
[Routing Layer] <-- decides: (quick cache / small model / RAG / agent)
↕
[Cache / Embedding Cache / Results Cache]
↕
[Retrieval Layer] -- Vector DB (pgvector/Chroma/Weaviate) + Reranker
↕
[Orchestration / Agent Layer] -- LangGraph / LangChain / AutoGen
↕
[Inference Layer] -- vLLM (self-host) or Managed (OpenAI/Anthropic)
↕
[Tooling Layer] -- DBs, Search, External APIs, Task Queues
↕
[Observability & Eval] -- Logs, Langfuse/Arize, CI Evals
↕
[Governance] -- Guard agents, Schema validators, Audit logs

**Notes:** model selection and cost-control logic usually live in the Routing Layer. Use separate networks/permissions for tools that access sensitive data.

---

## 2.4 Key trade-offs & decision matrix

- **Managed API vs Self-hosted**

  - _Managed_ = faster to ship, less infra, higher variable cost.
  - _Self-hosted_ = control & potentially lower long-term cost, higher ops.
  - _Decision:_ use managed for MVP & public features; self-host when you need privacy/compliance or heavy scale. :contentReference[oaicite:8]{index=8}

- **Large single model vs model ensemble**

  - _Ensemble_ (routing small→large) often reduces cost & latency while preserving quality for critical steps.

- **RAG store choice**

  - _pgvector_ fits teams that want SQL ops & ACID.
  - _Weaviate/Milvus_ when you need scale & specialized indexing.
  - _Chroma_ for quick local dev.

- **Agentic autonomy vs deterministic workflows**
  - Autonomous agents increase capability but add unpredictability and evaluation burden. Prefer _bounded autonomy_ with circuit-breakers.

---

## 2.5 Concrete recommendations (short checklist for your first 6 weeks)

1. **Prototype locally**: run an embedding→vector search→LLM flow (Chroma + Ollama).
2. **Add observability**: log all prompt inputs, retrieved chunks, model outputs, token usage. Use simple dashboards first (Prometheus + Grafana) then integrate Langfuse/Arize. :contentReference[oaicite:9]{index=9}
3. **Start with RAG (not fine-tuning)**: iterate quickly on retrieval quality and prompt templates.
4. **Design the routing layer early**: plan how requests map to small/medium/large models and caches.
5. **Implement output validation**: enforce JSON schema & source attribution before acting on results.
6. **Red-team & privacy review**: run prompt-injection tests and PII scans prior to production. :contentReference[oaicite:10]{index=10}

---

## 2.6 Expert-level reading & reference checklist (primary sources)

- **LLM Agents & Architecture**

  - Lilian Weng — _LLM-Powered Autonomous Agents_ (comprehensive, practical). :contentReference[oaicite:11]{index=11}
  - LangChain Blog — _How to think about agent frameworks_; _Design an Agent for Production_. :contentReference[oaicite:12]{index=12}

- **Production ML / MLOps**

  - Full Stack Deep Learning — course + production lectures (infra, monitoring, ops). :contentReference[oaicite:13]{index=13}
  - Google Cloud MLOps best practices and continuous delivery docs. :contentReference[oaicite:14]{index=14}
  - Chip Huyen — _Designing Machine Learning Systems_ (book; engineering practices). :contentReference[oaicite:15]{index=15}

- **Platform & Architecture Case Studies**
  - Uber Michelangelo (platform evolution & lessons for production ML). :contentReference[oaicite:16]{index=16}
  - OpenAI infrastructure security overview (guardrails & infra-level measures). :contentReference[oaicite:17]{index=17}
  - vLLM architecture (high-throughput inference server docs). :contentReference[oaicite:18]{index=18}

---

## 2.7 Short labs (practical, 1–3 days each)

**Lab A — Minimal production-ready RAG**

- Build: Next.js frontend + Go API + Chroma vector DB + Ollama embeddings + small LLM call.
- Add: Logging for prompt, retrieved chunk IDs, and token counts.
- Validate: Create a simple JSON-schema validator for responses.

**Lab B — Routing & Cost Control**

- Implement a routing service that:
  - Checks cache
  - If cache miss: compute embedding → retrieve top-k → estimate total tokens → pick model (small vs medium) → generate
- Measure cost delta between always-using-medium-model vs routing strategy.

**Lab C — Agent skeleton**

- Implement a simple Planner + Executor:
  - Planner: break a multi-step task into sub-tasks (use small LLM)
  - Executor: run sub-tasks (retrieval, API calls)
- Enforce: step limits, timeouts, and final JSON-schema validation.

---

## 2.8 Common anti-patterns (what senior teams regret)

- **No retrieval trace:** cannot debug hallucinations because retrieved chunks weren’t logged.
- **Everything in one big prompt:** huge token bills and brittle behavior.
- **No routing or caching:** unnecessary use of large models for trivial tasks.
- **Treating agents as autonomous without fail-safes:** leads to unpredictable and unsafe behavior.

---

## 2.9 Next sections in this chapter

- 2.10: Advanced Orchestration Patterns (graphs, planners, distributed agents)
- 2.11: LLMOps: CI/CD, eval suites, and regression testing for prompts & retrievals
- 2.12: Detailed cost-playbooks for cloud & hybrid deployments

---

## Quick copy-paste reading list (links)

- Lilian Weng — LLM-Powered Autonomous Agents: https://lilianweng.github.io/posts/2023-06-23-agent/
- LangChain — How to think about agent frameworks: https://blog.langchain.com/how-to-think-about-agent-frameworks/
- Google Cloud — MLOps & CI/CD for ML: https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning
- Chip Huyen — Designing Machine Learning Systems (book): https://huyenchip.com/ (book & resources).
- vLLM — server architecture & docs: https://docs.vllm.ai/en/latest/design/arch_overview.html
- Uber Michelangelo (platform lessons): https://www.uber.com/blog/michelangelo-machine-learning-platform/

---
