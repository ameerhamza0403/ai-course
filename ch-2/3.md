# Chapter 2: AI System Design & Architecture Foundations

## 2.2 AI System Design Patterns

While principles define the "why", **design patterns** define the "how". These are proven approaches to organizing AI components into systems that scale, remain maintainable, and adapt to future needs.

---

### 2.2.1 Layered Architecture

- **Description:** Organize system into layers:

| Layer | Purpose | Components |
|-------|---------|------------|
| **Data Layer** | Storage, ingestion, feature stores | Databases, data pipelines, feature stores |
| **Model Layer** | Training, experimentation, hyperparameter tuning | Training jobs, experiments, model registry |
| **Serving Layer** | REST/gRPC APIs, batch inference, streaming inference | APIs, inference services, load balancers |
| **Monitoring Layer** | Observability, logging, drift detection | Dashboards, alerting, metrics collection |
- **Benefit:** Clear boundaries, independent scaling per layer.
- **Example:** An e-commerce recommendation system with a dedicated **feature store** (data layer), **training jobs** (model layer), **recommendation API** (serving), and **drift dashboards** (monitoring).

---

### 2.2.2 Microservices + Model-as-a-Service

- **Description:** Deploy AI models as independent microservices with APIs.
- **Benefit:** Loose coupling, easier deployment, language-agnostic.
- **Challenge:** Managing dependencies (TensorFlow, PyTorch versions).
- **Solution:** Containers (Docker) + orchestration (Kubernetes).
- **Reference:** [ML Microservices by Spotify](https://engineering.atspotify.com/){:target="_blank"}

---

### 2.2.3 Event-Driven Architecture

- **Description:** Use event streams (Kafka, Pulsar) to trigger AI pipelines.
- **Benefit:** Real-time AI (fraud detection, anomaly detection, recommendations).
- **Example:** Transaction arrives → triggers fraud model → response within 100ms.

**Event-Driven Flow**:  
| Step | Event | Action | Response Time |
|------|-------|---------|---------------|
| 1 | Transaction arrives | Triggers fraud model | < 100ms |
| 2 | Model processes | Analyzes transaction | < 100ms |
| 3 | Response sent | Fraud decision returned | < 100ms |
- **Tools:** Kafka, AWS Kinesis, Apache Flink.
- **Reference:** [Confluent Blog – Event-Driven ML](https://www.confluent.io/blog/event-driven-architecture-for-ml/){:target="_blank"}

---

### 2.2.4 Data Pipeline Pattern

- **Description:** Separate raw, processed, and feature data pipelines.
- **Stages:**
  1. Ingestion (batch/streaming)
  2. Cleaning and validation
  3. Feature engineering
  4. Storage in feature store (Feast, Tecton)
- **Benefit:** Reproducibility, reusability across multiple models.
- **Reference:** [Feast Feature Store](https://feast.dev/){:target="_blank"}

---

### 2.2.5 Model Registry and Versioning

- **Description:** Keep track of every model version, metadata, and lineage.
- **Tools:** MLflow, Kubeflow, Vertex AI Model Registry.
- **Benefit:** Rollback, auditing, A/B testing made easier.
- **Example:** Model v1.2 → 91% accuracy → deployed in prod → after drift, rollback to v1.1.

**Model Versioning Flow**:  
| Step | Model Version | Status | Action |
|------|---------------|---------|---------|
| 1 | v1.2 | 91% accuracy | Deployed to production |
| 2 | v1.2 | Performance drift detected | Monitor and evaluate |
| 3 | v1.1 | Stable performance | Rollback from v1.2 |

---

### 2.2.6 Online vs Batch Inference

- **Batch inference:**
  - Run periodically (e.g., daily recommendations).
  - Cheap and efficient for non-time-critical tasks.
- **Online inference:**
  - Real-time, low latency (<100ms).
  - Needed for chatbots, fraud detection, search ranking.
- **Hybrid approach:** Precompute with batch → refine with online.
- **Reference:** [Uber’s Batch vs Real-Time ML](https://eng.uber.com/michelangelo/){:target="_blank"}

---

### 2.2.7 Model Deployment Strategies

- **Blue/Green Deployments:** Run two environments → switch traffic gradually.
- **Canary Deployments:** Route small % of users to new model.
- **Shadow Deployments:** Run new model in parallel, don’t serve responses.
- **Benefit:** Minimize risk, safe rollout of new models.
- **Reference:** [Google Cloud AI Deployment Patterns](https://cloud.google.com/architecture){:target="_blank"}

---

### 2.2.8 Orchestration Pattern

- **Description:** Use orchestrators (Airflow, Kubeflow, Prefect) to schedule and manage pipelines.
- **Benefit:** Automation, reproducibility, dependency management.
- **Example:** Nightly pipeline → data validation → feature generation → retrain → deploy new model.
- **Reference:** [Apache Airflow for ML](https://airflow.apache.org/){:target="_blank"}

---

### 2.2.9 Human-in-the-Loop (HITL)

- **Description:** Incorporate human review in AI decision-making.
- **Example:** Content moderation system → AI flags → human verifies edge cases.
- **Benefit:** Improves quality, reduces bias, ensures compliance.
- **Reference:** [Google Research – Human in the Loop AI](https://research.google/){:target="_blank"}

---

### 2.2.10 Federated Learning Pattern

- **Description:** Train models across multiple devices/orgs without centralizing data.
- **Example:** Mobile keyboards learning from typing patterns without uploading raw text.
- **Benefit:** Privacy-preserving, scalable, compliant.
- **Reference:** [Google Federated Learning](https://ai.googleblog.com/2017/04/federated-learning-collaborative.html){:target="_blank"}

---

✅ **Summary:**  
AI design patterns shape how systems are **structured, deployed, and scaled**. From **layered architectures** to **event-driven pipelines**, from **model registries** to **federated learning**, these patterns are the building blocks of enterprise-grade AI systems.


